# Ethical AI Store
Collection of links for Ethical AI, Fairness & Transperency libraries.

Note this is mostly Python centric but welcome additions regardless of the language.


**Deon**
- **Summary**: An ethics checklist for data scientists
- ****Repo**: https://github.com/drivendataorg/deon
- ****Docs**: https://deon.drivendata.org/

**Alibi**
- **Summary**: Alibi is an open source Python library aimed at machine learning model inspection and interpretation
- **Repo**: https://github.com/SeldonIO/alibi
- **Docs**: https://**Docs**.seldon.io/projects/alibi/en/stable


**AI Fairness 360**
- **Summary**: The AI Fairness 360 toolkit is an extensible open-source library containg techniques developed by the research community to help detect and mitigate bias in machine learning models throughout the AI application lifecycle.
- **Repo**: https://github.com/Trusted-AI/AIF360
- **Docs**: https://aif360.mybluemix.net/


**Interpret**
- **Summary**: InterpretML is an open-source package that incorporates state-of-the-art machine learning interpretability techniques under one roof.
- **Repo**: https://github.com/interpretml/interpret
- **Docs**: N/A


**Fairlean**
- **Summary**: Fairlearn is a Python package that empowers developers of artificial intelligence (AI) systems to assess their system's fairness and mitigate any observed unfairness issues
- **Repo**: https://github.com/fairlearn/fairlearn
- **Docs**: https://fairlearn.github.io/master/index.html


**Fairness Indicators**
- **Summary**: Fairness Indicators is designed to support teams in evaluating, improving, and comparing models for fairness concerns in partnership with the broader Tensorflow toolkit.
- **Repo**: https://github.com/tensorflow/fairness-indicators
- **Docs**: https://www.tensorflow.org/tfx/guide/fairness_indicators


**FAT Forensics**
- **Summary**: FAT Forensics is a Python toolkit for evaluating Fairness, Accountability and Transparency of Artificial Intelligence systems
- **Repo**: https://github.com/fat-forensics/fat-forensics
- **Docs**: https://fat-forensics.org/


**Anchor**
- **Summary**: An anchor explanation is a rule that sufficiently “anchors” the prediction locally – such that changes to the rest of the feature values of the instance do not matter. In other words, for instances on which the anchor holds, the prediction is (almost) always the same
- **Repo**: https://github.com/marcotcr/anchor
- **Paper**: https://homes.cs.washington.edu/~marcotcr/aaai18.pdf


**PyCEbox**
- **Summary**: A Python implementation of individual conditional expecation plots inspired by R's [ICEbox](https://cran.r-project.org/web/packages/ICEbox/index.html)
- **Repo**: https://github.com/AustinRochford/PyCEbox
- **Docs**: http://austinrochford.github.io/PyCEbox/docs/


**What-If Tool**
- **Summary**: The What-If Tool (WIT) provides an easy-to-use interface for expanding understanding of a black-box classification or regression ML model
- **Repo**: https://github.com/pair-code/what-if-tool
- **Docs**: https://pair-code.github.io/what-if-tool/


**LIME**
- **Summary**: This project is about explaining what machine learning classifiers (or models) are doing
- **Repo**: https://github.com/marcotcr/lime
- **Paper**: https://arxiv.org/abs/1602.04938


**SHAP**
- **Summary**: SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model
- **Repo**: https://github.com/slundberg/shap


**Skater**
- **Summary**: Skater is a unified framework to enable Model Interpretation for all forms of model to help one build an Interpretable machine learning system often needed for real world use-cases
- **Repo**: https://github.com/oracle/Skater
- **Docs**: https://oracle.github.io/Skater/index.html


**Black Box Auditing**
- **Summary**: This repository contains a sample implementation of Gradient Feature Auditing (GFA) meant to be generalizable to most datasets.
- **Repo**: https://github.com/algofairness/BlackBoxAuditing
- **Paper**: https://arxiv.org/abs/1412.3756


**Fairness Comparison**
- **Summary**: A comparative study of fairness-enhancing interventions in machine learning. This repository is meant to facilitate the benchmarking of fairness aware machine learning algorithms.
- **Repo**: https://github.com/algofairness/fairness-comparison
- **Paper**: https://arxiv.org/abs/1802.04422


**FairTest**
- **Summary**: FairTest enables developers or auditing entities to discover and test for unwarranted associations between an algorithm's outputs and certain user subpopulations identified by protected features.
- **Repo**: https://github.com/columbia/fairtest
- **Docs**: *N/A*


**FairML**
- **Summary**: FairML is a python toolbox auditing the machine learning models for bias.
- **Repo**: https://github.com/adebayoj/fairml
- **Docs**: *N/A*
