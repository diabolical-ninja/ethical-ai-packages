# Ethical AI Packages
Collection of links for Ethical AI, Fairness & Transperency libraries.

Note this is mostly Python centric but welcome additions regardless of the language.


**Deon**
- **Summary**: An ethics checklist for data scientists
- **Repo**: https://github.com/drivendataorg/deon
- **Docs**: https://deon.drivendata.org/

**Alibi**
- **Summary**: Alibi is an open source Python library aimed at machine learning model inspection and interpretation
- **Repo**: https://github.com/SeldonIO/alibi
- **Docs**: https://**Docs**.seldon.io/projects/alibi/en/stable


**AI Fairness 360**
- **Summary**: The AI Fairness 360 toolkit is an extensible open-source library containg techniques developed by the research community to help detect and mitigate bias in machine learning models throughout the AI application lifecycle.
- **Repo**: https://github.com/Trusted-AI/AIF360
- **Docs**: https://aif360.mybluemix.net/


**AI Explainability 360**
- **Summary**: The AI Explainability 360 toolkit is an open-source library that supports interpretability and explainability of datasets and machine learning models
- **Repo**: https://github.com/Trusted-AI/AIX360
- **Docs**: http://aix360.mybluemix.net/


**Interpret**
- **Summary**: InterpretML is an open-source package that incorporates state-of-the-art machine learning interpretability techniques under one roof.
- **Repo**: https://github.com/interpretml/interpret
- **Docs**: N/A


**Fairlean**
- **Summary**: Fairlearn is a Python package that empowers developers of artificial intelligence (AI) systems to assess their system's fairness and mitigate any observed unfairness issues
- **Repo**: https://github.com/fairlearn/fairlearn
- **Docs**: https://fairlearn.github.io/master/index.html


**Fairness Indicators**
- **Summary**: Fairness Indicators is designed to support teams in evaluating, improving, and comparing models for fairness concerns in partnership with the broader Tensorflow toolkit.
- **Repo**: https://github.com/tensorflow/fairness-indicators
- **Docs**: https://www.tensorflow.org/tfx/guide/fairness_indicators


**FAT Forensics**
- **Summary**: FAT Forensics is a Python toolkit for evaluating Fairness, Accountability and Transparency of Artificial Intelligence systems
- **Repo**: https://github.com/fat-forensics/fat-forensics
- **Docs**: https://fat-forensics.org/


**Anchor**
- **Summary**: An anchor explanation is a rule that sufficiently “anchors” the prediction locally – such that changes to the rest of the feature values of the instance do not matter. In other words, for instances on which the anchor holds, the prediction is (almost) always the same
- **Repo**: https://github.com/marcotcr/anchor
- **Paper**: https://homes.cs.washington.edu/~marcotcr/aaai18.pdf


**PyCEbox**
- **Summary**: A Python implementation of individual conditional expecation plots inspired by R's [ICEbox](https://cran.r-project.org/web/packages/ICEbox/index.html)
- **Repo**: https://github.com/AustinRochford/PyCEbox
- **Docs**: http://austinrochford.github.io/PyCEbox/docs/


**What-If Tool**
- **Summary**: The What-If Tool (WIT) provides an easy-to-use interface for expanding understanding of a black-box classification or regression ML model
- **Repo**: https://github.com/pair-code/what-if-tool
- **Docs**: https://pair-code.github.io/what-if-tool/


**LIME**
- **Summary**: This project is about explaining what machine learning classifiers (or models) are doing
- **Repo**: https://github.com/marcotcr/lime
- **Paper**: https://arxiv.org/abs/1602.04938


**SHAP**
- **Summary**: SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain the output of any machine learning model
- **Repo**: https://github.com/slundberg/shap


**Yellowbrick**
- **Summary**: Yellowbrick extends the Scikit-Learn API to make model selection and hyperparameter tuning easier. Under the hood, it’s using Matplotlib.
- **Repo**: https://github.com/DistrictDataLabs/yellowbrick
- **Docs**: https://www.scikit-yb.org/en/latest/


**Tensorflow Fairness Indicators**
- **Summary**: Fairness Indicators is designed to support teams in evaluating, improving, and comparing models for fairness concerns in partnership with the broader Tensorflow toolkit.
- **Repo**: https://github.com/tensorflow/fairness-indicators
- **Docs**: https://www.tensorflow.org/responsible_ai


**PyCM**
- **Summary**: PyCM is a multi-class confusion matrix library written in Python that supports both input data vectors and direct matrix, and a proper tool for post-classification model evaluation that supports most classes and overall statistics parameters.
- **Repo**: https://github.com/sepandhaghighi/pycm
- **Docs**: https://www.pycm.ir/


**ELI5**
- **Summary**: A library for debugging/inspecting machine learning classifiers and explaining their predictions
- **Repo**: https://github.com/TeamHG-Memex/eli5
- **Docs**: http://eli5.readthedocs.io/


**Skater**
- **Summary**: Skater is a unified framework to enable Model Interpretation for all forms of model to help one build an Interpretable machine learning system often needed for real world use-cases
- **Repo**: https://github.com/oracle/Skater
- **Docs**: https://oracle.github.io/Skater/index.html


**Black Box Auditing**
- **Summary**: This repository contains a sample implementation of Gradient Feature Auditing (GFA) meant to be generalizable to most datasets.
- **Repo**: https://github.com/algofairness/BlackBoxAuditing
- **Paper**: https://arxiv.org/abs/1412.3756


**Fairness Comparison**
- **Summary**: A comparative study of fairness-enhancing interventions in machine learning. This repository is meant to facilitate the benchmarking of fairness aware machine learning algorithms.
- **Repo**: https://github.com/algofairness/fairness-comparison
- **Paper**: https://arxiv.org/abs/1802.04422


**FairTest**
- **Summary**: FairTest enables developers or auditing entities to discover and test for unwarranted associations between an algorithm's outputs and certain user subpopulations identified by protected features.
- **Repo**: https://github.com/columbia/fairtest
- **Docs**: *N/A*


**FairML**
- **Summary**: FairML is a python toolbox auditing the machine learning models for bias.
- **Repo**: https://github.com/adebayoj/fairml
- **Docs**: *N/A*


**ml-fairness-gym**
- **Summary**: ML-fairness-gym is a set of components for building simple simulations that explore the potential long-run impacts of deploying machine learning-based decision systems in social environments.
- **Repo**: https://github.com/google/ml-fairness-gym/
- **Docs**: https://github.com/google/ml-fairness-gym/tree/master/docs
- **Paper**: https://dl.acm.org/doi/pdf/10.1145/3351095.3372878


**TensorFlow Constrained Optimization (TFCO)**
- **Summary**: TFCO is a library for optimizing inequality-constrained problems in TensorFlow 1.14 and later (including TensorFlow 2).
- **Repo**: https://github.com/google-research/tensorflow_constrained_optimization
- **Paper**: http://proceedings.mlr.press/v98/cotter19a/cotter19a.pdf


**Dalex**
- **Summary**: The DALEX package xrays any model and helps to explore and explain its behaviour, helps to understand how complex models are working.
- **Repo**: https://github.com/ModelOriented/DALEX
- **Docs**: https://dalex.drwhy.ai/


**Aequitas**
- **Summary**: Aequitas is an open-source bias audit toolkit for data scientists, machine learning researchers, and policymakers to audit machine learning models for discrimination and bias, and to make informed and equitable decisions around developing and deploying predictive tools.
- **Repo**: https://github.com/dssg/aequitas
- **Docs**: http://www.datasciencepublicpolicy.org/projects/aequitas/
